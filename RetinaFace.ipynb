{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Сеть является устойчивым одностадийным детектором, предоставляющим попиксельную локализацию для лиц разного масштаба.\nДанная реализация взята с https://github.com/peteryuX/retinaface-tf2.git\n\n\n![](https://translate.google.com/website?sl=en&tl=ru&prev=search&u=https://i2.wp.com/sefiks.com/wp-content/uploads/2021/04/retinaface-structure.png?ssl%3D1)\n\nАрхитектура сети состоит из 3 основных частей, каждая из которых имеет своё назначение:\n\na) Backbone – основная (базовая) сеть, служащая для извлечения признаков из поступающего на вход изображения. Данная часть сети является вариативной и в её основу могут входить классификационные нейросети, такие как ResNet, VGG, EfficientNet и другие;\n\nb) Feature Pyramid Net (FPN) – свёрточная нейронная сеть, построенная в виде пирамиды, служащая для объединения достоинств карт признаков нижних и верхних уровней сети, первые имеют высокое разрешение, но низкую семантическую, обобщающую способность; вторые — наоборот;\n\nс) Regression Subnet – подсеть, извлекающая из FPN информацию о координатах объектов на изображении, решая задачу регрессии.","metadata":{"_uuid":"b0244a43-2ec4-440c-a765-42d02aa797e5","_cell_guid":"60589c51-6a1f-4321-9ae9-9f67878dc7d3","trusted":true}},{"cell_type":"code","source":"#подключим необходимые библиотеки\nimport os\nimport zipfile\nfrom dataclasses import dataclass\n\nfrom absl import logging\nimport tqdm\nimport random\nimport tensorflow as tf\nimport numpy as np\nfrom six import BytesIO\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport yaml\nimport sys\nimport time\n\nfrom tensorflow.keras.utils import plot_model\n\nimport copy\n\nimport numpy as np\n\n%matplotlib inline","metadata":{"_uuid":"d22f1f3c-7f74-4b5b-aec5-5dc458a67909","_cell_guid":"66a73ae0-5fa6-4e62-861a-44f720e56140","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:45.718222Z","iopub.execute_input":"2022-01-10T10:45:45.718515Z","iopub.status.idle":"2022-01-10T10:45:45.727952Z","shell.execute_reply.started":"2022-01-10T10:45:45.718482Z","shell.execute_reply":"2022-01-10T10:45:45.727235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Путь к базовому файлу конфигурации обучения (взят с гитхаба вместе с исходниками)\nCFG_PATH = f'../input/configresnet/retinaface_res50.yaml'","metadata":{"_uuid":"893a947e-9b39-4c8c-aa61-84833c391236","_cell_guid":"6d1ed5ef-6147-4300-b305-581e900ff00d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:45.881726Z","iopub.execute_input":"2022-01-10T10:45:45.882494Z","iopub.status.idle":"2022-01-10T10:45:45.887893Z","shell.execute_reply.started":"2022-01-10T10:45:45.882453Z","shell.execute_reply":"2022-01-10T10:45:45.887054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"54c2905c-689d-43ed-981b-a77dfaa9ae65","_cell_guid":"80ed0049-2642-44b3-8da8-126752df8c39","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Обучать будем на WIDER FACE. Датасет должен быть в формате tfrecord. Подготовленный датасет можно взять здесь:**\nhttps://www.kaggle.com/b1qkoa/widerface","metadata":{"_uuid":"8c33789c-f72e-46f9-86f7-4dcea7222c73","_cell_guid":"bacde83d-d8f9-4e96-a3bf-e76d83a39f0a","trusted":true}},{"cell_type":"code","source":"#Планировщик обучения\nclass PiecewiseConstantWarmUpDecay(\n        tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"A LearningRateSchedule wiht warm up schedule.\n    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n\n    def __init__(self, boundaries, values, warmup_steps, min_lr,\n                 name=None):\n        super(PiecewiseConstantWarmUpDecay, self).__init__()\n\n        if len(boundaries) != len(values) - 1:\n            raise ValueError(\n                    \"The length of boundaries should be 1 less than the\"\n                    \"length of values\")\n\n        self.boundaries = boundaries\n        self.values = values\n        self.name = name\n        self.warmup_steps = warmup_steps\n        self.min_lr = min_lr\n\n    def __call__(self, step):\n        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n            pred_fn_pairs = []\n            warmup_steps = self.warmup_steps\n            boundaries = self.boundaries\n            values = self.values\n            min_lr = self.min_lr\n\n            pred_fn_pairs.append(\n                (step <= warmup_steps,\n                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n            pred_fn_pairs.append(\n                (tf.logical_and(step <= boundaries[0],\n                                step > warmup_steps),\n                 lambda: tf.constant(values[0])))\n            pred_fn_pairs.append(\n                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n\n            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n                                    values[1:-1]):\n                pred = (step > low) & (step <= high)\n                pred_fn_pairs.append((pred, lambda v=v: tf.constant(v)))\n\n            # The default isn't needed here because our conditions are mutually\n            # exclusive and exhaustive, but tf.case requires it.\n            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n                           exclusive=True)\n\n    def get_config(self):\n        return {\n                \"boundaries\": self.boundaries,\n                \"values\": self.values,\n                \"warmup_steps\": self.warmup_steps,\n                \"min_lr\": self.min_lr,\n                \"name\": self.name\n        }\n\n    \ndef MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n                      warmup_steps=0., min_lr=0.,\n                      name='MultiStepWarmUpLR'):\n    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n\n    assert warmup_steps <= lr_steps[0], f\"{warmup_steps} <= {lr_steps[0]}\"\n    assert min_lr <= initial_learning_rate\n    lr_steps_value = [initial_learning_rate]\n    for _ in range(len(lr_steps)):\n        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n    return PiecewiseConstantWarmUpDecay(\n        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n        min_lr=min_lr)","metadata":{"_uuid":"9fdc8c44-fcff-4d46-943a-31c765e79e2d","_cell_guid":"5057a50c-9a8f-4199-b0ba-3536b20fe7ff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:46.322885Z","iopub.execute_input":"2022-01-10T10:45:46.32339Z","iopub.status.idle":"2022-01-10T10:45:46.354506Z","shell.execute_reply.started":"2022-01-10T10:45:46.323337Z","shell.execute_reply":"2022-01-10T10:45:46.353496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#используемые функции потерь\ndef _smooth_l1_loss(y_true, y_pred):\n    t = tf.abs(y_pred - y_true)\n    return tf.where(t < 1, 0.5 * t ** 2, t - 0.5)\n\ndef MultiBoxLoss(num_class=2, neg_pos_ratio=3):\n    \"\"\"multi-box loss\"\"\"\n    def multi_box_loss(y_true, y_pred):\n        num_batch = tf.shape(y_true)[0]\n        num_prior = tf.shape(y_true)[1]\n\n        loc_pred = tf.reshape(y_pred[0], [num_batch * num_prior, 4])\n        landm_pred = tf.reshape(y_pred[1], [num_batch * num_prior, 10])\n        class_pred = tf.reshape(y_pred[2], [num_batch * num_prior, num_class])\n        loc_true = tf.reshape(y_true[..., :4], [num_batch * num_prior, 4])\n        landm_true = tf.reshape(y_true[..., 4:14], [num_batch * num_prior, 10])\n        landm_valid = tf.reshape(y_true[..., 14], [num_batch * num_prior, 1])\n        class_true = tf.reshape(y_true[..., 15], [num_batch * num_prior, 1])\n\n        # define filter mask: class_true = 1 (pos), 0 (neg), -1 (ignore)\n        #                     landm_valid = 1 (w landm), 0 (w/o landm)\n        mask_pos = tf.equal(class_true, 1)\n        mask_neg = tf.equal(class_true, 0)\n        mask_landm = tf.logical_and(tf.equal(landm_valid, 1), mask_pos)\n\n        # landm loss (smooth L1)\n        mask_landm_b = tf.broadcast_to(mask_landm, tf.shape(landm_true))\n        loss_landm = _smooth_l1_loss(tf.boolean_mask(landm_true, mask_landm_b),\n                                     tf.boolean_mask(landm_pred, mask_landm_b))\n        loss_landm = tf.reduce_mean(loss_landm)\n\n        # localization loss (smooth L1)\n        mask_pos_b = tf.broadcast_to(mask_pos, tf.shape(loc_true))\n        loss_loc = _smooth_l1_loss(tf.boolean_mask(loc_true, mask_pos_b),\n                                   tf.boolean_mask(loc_pred, mask_pos_b))\n        loss_loc = tf.reduce_mean(loss_loc)\n\n        # classification loss (crossentropy)\n        # 1. compute max conf across batch for hard negative mining\n        loss_class = tf.where(mask_neg,\n                              1 - class_pred[:, 0][..., tf.newaxis], 0)\n\n        # 2. hard negative mining\n        loss_class = tf.reshape(loss_class, [num_batch, num_prior])\n        loss_class_idx = tf.argsort(loss_class, axis=1, direction='DESCENDING')\n        loss_class_idx_rank = tf.argsort(loss_class_idx, axis=1)\n        mask_pos_per_batch = tf.reshape(mask_pos, [num_batch, num_prior])\n        num_pos_per_batch = tf.reduce_sum(\n                tf.cast(mask_pos_per_batch, tf.float32), 1, keepdims=True)\n        num_pos_per_batch = tf.maximum(num_pos_per_batch, 1)\n        num_neg_per_batch = tf.minimum(neg_pos_ratio * num_pos_per_batch,\n                                       tf.cast(num_prior, tf.float32) - 1)\n        mask_hard_neg = tf.reshape(\n            tf.cast(loss_class_idx_rank, tf.float32) < num_neg_per_batch,\n            [num_batch * num_prior, 1])\n\n        # 3. classification loss including positive and negative examples\n        loss_class_mask = tf.logical_or(mask_pos, mask_hard_neg)\n        loss_class_mask_b = tf.broadcast_to(loss_class_mask,\n                                            tf.shape(class_pred))\n        filter_class_true = tf.boolean_mask(tf.cast(mask_pos, tf.float32),\n                                            loss_class_mask)\n        filter_class_pred = tf.boolean_mask(class_pred, loss_class_mask_b)\n        filter_class_pred = tf.reshape(filter_class_pred, [-1, num_class])\n        loss_class = tf.keras.losses.sparse_categorical_crossentropy(\n            y_true=filter_class_true, y_pred=filter_class_pred)\n        loss_class = tf.reduce_mean(loss_class)\n\n        return loss_loc, loss_landm, loss_class\n\n    return multi_box_loss","metadata":{"_uuid":"3b5faf22-3e19-46b6-aa4f-18d39a315b45","_cell_guid":"52cdb51d-d1f4-4dfb-b9c6-d5d8c60ed01b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:46.491578Z","iopub.execute_input":"2022-01-10T10:45:46.494501Z","iopub.status.idle":"2022-01-10T10:45:46.528835Z","shell.execute_reply.started":"2022-01-10T10:45:46.494456Z","shell.execute_reply":"2022-01-10T10:45:46.527872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ключевые точки и рамки\ndef _encode_bbox(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    \"\"\"\n\n    # dist b/t match center and prior's center\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = tf.math.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\ndef _encode_landm(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 10].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded landm (tensor), Shape: [num_priors, 10]\n    \"\"\"\n\n    # dist b/t match center and prior's center\n    matched = tf.reshape(matched, [tf.shape(matched)[0], 5, 2])\n    priors = tf.broadcast_to(\n        tf.expand_dims(priors, 1), [tf.shape(matched)[0], 5, 4])\n    g_cxcy = matched[:, :, :2] - priors[:, :, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, :, 2:])\n    # g_cxcy /= priors[:, :, 2:]\n    g_cxcy = tf.reshape(g_cxcy, [tf.shape(g_cxcy)[0], -1])\n    # return target for smooth_l1_loss\n    return g_cxcy\n\ndef _point_form(boxes):\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return tf.concat((boxes[:, :2] - boxes[:, 2:] / 2,\n                      boxes[:, :2] + boxes[:, 2:] / 2), axis=1)\n\n\ndef _intersect(box_a, box_b):\n    \"\"\" We resize both tensors to [A,B,2]:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    \"\"\"\n    A = tf.shape(box_a)[0]\n    B = tf.shape(box_b)[0]\n    max_xy = tf.minimum(\n        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n    min_xy = tf.maximum(\n        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n    inter = tf.maximum((max_xy - min_xy), tf.zeros_like(max_xy - min_xy))\n    return inter[:, :, 0] * inter[:, :, 1]\n\ndef _jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    \"\"\"\n    inter = _intersect(box_a, box_b)\n    area_a = tf.broadcast_to(\n        tf.expand_dims(\n            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n        tf.shape(inter))  # [A,B]\n    area_b = tf.broadcast_to(\n        tf.expand_dims(\n            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n        tf.shape(inter))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n###############################################################################\n#   Tensorflow Encoding                                                       #\n###############################################################################\ndef encode_tf(labels, priors, match_thresh, ignore_thresh,\n              variances=[0.1, 0.2]):\n    \"\"\"tensorflow encoding\"\"\"\n    assert ignore_thresh <= match_thresh\n    priors = tf.cast(priors, tf.float32)\n    bbox = labels[:, :4]\n    landm = labels[:, 4:-1]\n    landm_valid = labels[:, -1]  # 1: with landm, 0: w/o landm.\n\n    # jaccard index\n    overlaps = _jaccard(bbox, _point_form(priors))\n\n    # (Bipartite Matching)\n    # [num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = tf.math.top_k(overlaps, k=1)\n    best_prior_overlap = best_prior_overlap[:, 0]\n    best_prior_idx = best_prior_idx[:, 0]\n\n    # [num_priors] best ground truth for each prior\n    overlaps_t = tf.transpose(overlaps)\n    best_truth_overlap, best_truth_idx = tf.math.top_k(overlaps_t, k=1)\n    best_truth_overlap = best_truth_overlap[:, 0]\n    best_truth_idx = best_truth_idx[:, 0]\n\n    # ensure best prior\n    def _loop_body(i, bt_idx, bt_overlap):\n        bp_mask = tf.one_hot(best_prior_idx[i], tf.shape(bt_idx)[0])\n        bp_mask_int = tf.cast(bp_mask, tf.int32)\n        new_bt_idx = bt_idx * (1 - bp_mask_int) + bp_mask_int * i\n        bp_mask_float = tf.cast(bp_mask, tf.float32)\n        new_bt_overlap = bt_overlap * (1 - bp_mask_float) + bp_mask_float * 2\n        return tf.cond(best_prior_overlap[i] > match_thresh,\n                       lambda: (i + 1, new_bt_idx, new_bt_overlap),\n                       lambda: (i + 1, bt_idx, bt_overlap))\n    _, best_truth_idx, best_truth_overlap = tf.while_loop(\n        lambda i, bt_idx, bt_overlap: tf.less(i, tf.shape(best_prior_idx)[0]),\n        _loop_body, [tf.constant(0), best_truth_idx, best_truth_overlap])\n\n    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n    matches_landm = tf.gather(landm, best_truth_idx)  # [num_priors, 10]\n    matches_landm_v = tf.gather(landm_valid, best_truth_idx)  # [num_priors]\n\n    loc_t = _encode_bbox(matches_bbox, priors, variances)\n    landm_t = _encode_landm(matches_landm, priors, variances)\n    landm_valid_t = tf.cast(matches_landm_v > 0, tf.float32)\n    conf_t = tf.cast(best_truth_overlap > match_thresh, tf.float32)\n    conf_t = tf.where(\n        tf.logical_and(best_truth_overlap < match_thresh,\n                       best_truth_overlap > ignore_thresh),\n        tf.ones_like(conf_t) * -1, conf_t)    # 1: pos, 0: neg, -1: ignore\n\n    return tf.concat([loc_t, landm_t, landm_valid_t[..., tf.newaxis],\n                      conf_t[..., tf.newaxis]], axis=1)\n\n\ndef decode_tf(labels, priors, variances=[0.1, 0.2]):\n    \"\"\"tensorflow decoding\"\"\"\n    bbox = _decode_bbox(labels[:, :4], priors, variances)\n    landm = _decode_landm(labels[:, 4:14], priors, variances)\n    landm_valid = labels[:, 14][:, tf.newaxis]\n    conf = labels[:, 15][:, tf.newaxis]\n\n    return tf.concat([bbox, landm, landm_valid, conf], axis=1)\n\n\ndef _decode_bbox(pre, priors, variances=[0.1, 0.2]):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    \"\"\"\n    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\n    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\n\n    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)\n\ndef _decode_landm(pre, priors, variances=[0.1, 0.2]):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded landm predictions\n    \"\"\"\n    landms = tf.concat(\n        [priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n         priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n         priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n         priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n         priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:]], axis=1)\n    return landms\n\n\n###############################################################################\n#   Tensorflow / Numpy Priors                                                 #\n###############################################################################\nimport math\nfrom itertools import product as product\n\ndef prior_box(image_sizes, min_sizes, steps, clip=False):\n    \"\"\"prior box\"\"\"\n    feature_maps = [\n        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n        for step in steps]\n\n    anchors = []\n    for k, f in enumerate(feature_maps):\n        for i, j in product(range(f[0]), range(f[1])):\n            for min_size in min_sizes[k]:\n                s_kx = min_size / image_sizes[1]\n                s_ky = min_size / image_sizes[0]\n                cx = (j + 0.5) * steps[k] / image_sizes[1]\n                cy = (i + 0.5) * steps[k] / image_sizes[0]\n                anchors += [cx, cy, s_kx, s_ky]\n\n    output = np.asarray(anchors).reshape([-1, 4])\n\n    if clip:\n        output = np.clip(output, 0, 1)\n\n    return output\n\ndef prior_box_tf(image_sizes, min_sizes, steps, clip=False):\n    \"\"\"prior box\"\"\"\n    image_sizes = tf.cast(tf.convert_to_tensor(image_sizes), tf.float32)\n    feature_maps = tf.math.ceil(\n        tf.reshape(image_sizes, [1, 2]) /\n        tf.reshape(tf.cast(steps, tf.float32), [-1, 1]))\n\n    anchors = []\n    for k in range(len(min_sizes)):\n        grid_x, grid_y = _meshgrid_tf(tf.range(feature_maps[k][1]),\n                                      tf.range(feature_maps[k][0]))\n        cx = (grid_x + 0.5) * steps[k] / image_sizes[1]\n        cy = (grid_y + 0.5) * steps[k] / image_sizes[0]\n        cxcy = tf.stack([cx, cy], axis=-1)\n        cxcy = tf.reshape(cxcy, [-1, 2])\n        cxcy = tf.repeat(cxcy, repeats=tf.shape(min_sizes[k])[0], axis=0)\n\n        sx = min_sizes[k] / image_sizes[1]\n        sy = min_sizes[k] / image_sizes[0]\n        sxsy = tf.stack([sx, sy], 1)\n        sxsy = tf.repeat(sxsy[tf.newaxis],\n                         repeats=tf.shape(grid_x)[0] * tf.shape(grid_x)[1],\n                         axis=0)\n        sxsy = tf.reshape(sxsy, [-1, 2])\n\n        anchors.append(tf.concat([cxcy, sxsy], 1))\n\n    output = tf.concat(anchors, axis=0)\n\n    if clip:\n        output = tf.clip_by_value(output, 0, 1)\n\n    return output\n\n\ndef _meshgrid_tf(x, y):\n    \"\"\" workaround solution of the tf.meshgrid() issue:\n        https://github.com/tensorflow/tensorflow/issues/34470\"\"\"\n    grid_shape = [tf.shape(y)[0], tf.shape(x)[0]]\n    grid_x = tf.broadcast_to(tf.reshape(x, [1, -1]), grid_shape)\n    grid_y = tf.broadcast_to(tf.reshape(y, [-1, 1]), grid_shape)\n    return grid_x, grid_y","metadata":{"_uuid":"26789cee-5cb7-4bbe-b40a-15d3533d9002","_cell_guid":"fd86e48a-9abf-4d2e-ba46-bf8f537e043d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:46.819403Z","iopub.execute_input":"2022-01-10T10:45:46.81983Z","iopub.status.idle":"2022-01-10T10:45:46.879828Z","shell.execute_reply.started":"2022-01-10T10:45:46.819796Z","shell.execute_reply":"2022-01-10T10:45:46.878933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#составные части модели\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications import ResNet50, MobileNetV3Small  #MobileNetV2\nfrom tensorflow.keras.layers import Input, Conv2D, ReLU, LeakyReLU\n\nimport pickle\n\ndef _regularizer(weights_decay):\n    \"\"\"l2 regularizer\"\"\"\n    return tf.keras.regularizers.l2(weights_decay)\n\ndef _kernel_init(scale=1.0, seed=None):\n    \"\"\"He normal initializer\"\"\"\n    return tf.keras.initializers.he_normal()\n\nclass BatchNormalization(tf.keras.layers.BatchNormalization):\n    \"\"\"Make trainable=False freeze BN for real (the og version is sad).\n       ref: https://github.com/zzh8829/yolov3-tf2\n    \"\"\"\n    def __init__(self, axis=-1, momentum=0.9, epsilon=1e-5, center=True,\n                 scale=True, name=None, **kwargs):\n        super(BatchNormalization, self).__init__(\n            axis=axis, momentum=momentum, epsilon=epsilon, center=center,\n            scale=scale, name=name, **kwargs)\n\n    def call(self, x, training=False):\n        if training is None:\n            training = tf.constant(False)\n        training = tf.logical_and(training, self.trainable)\n\n        return super().call(x, training)\n    \n\nclass ConvUnit(tf.keras.layers.Layer):\n    \"\"\"Conv + BN + Act\"\"\"\n    def __init__(self, f, k, s, wd, act=None, name='ConvBN', **kwargs):\n        super(ConvUnit, self).__init__(name=name, **kwargs)\n        self.conv = Conv2D(filters=f, kernel_size=k, strides=s, padding='same',\n                           kernel_initializer=_kernel_init(),\n                           kernel_regularizer=_regularizer(wd),\n                           use_bias=False, name='conv')\n        self.bn = BatchNormalization(name='bn')\n\n        if act is None:\n            self.act_fn = tf.identity\n        elif act == 'relu':\n            self.act_fn = ReLU()\n        elif act == 'lrelu':\n            self.act_fn = LeakyReLU(0.1)\n        else:\n            raise NotImplementedError(\n                'Activation function type {} is not recognized.'.format(act))\n\n    def call(self, x):\n        return self.act_fn(self.bn(self.conv(x)))\n\nclass FPN(tf.keras.layers.Layer):\n    \"\"\"Feature Pyramid Network\"\"\"\n    def __init__(self, out_ch, wd, name='FPN', **kwargs):\n        super(FPN, self).__init__(name=name, **kwargs)\n        act = 'relu'\n        if (out_ch <= 64):\n            act = 'lrelu'\n\n        self.output1 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=act)\n        self.output2 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=act)\n        self.output3 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=act)\n        self.merge1 = ConvUnit(f=out_ch, k=3, s=1, wd=wd, act=act)\n        self.merge2 = ConvUnit(f=out_ch, k=3, s=1, wd=wd, act=act)\n\n    def call(self, x):\n        output1 = self.output1(x[0])  # [80, 80, out_ch]\n        output2 = self.output2(x[1])  # [40, 40, out_ch]\n        output3 = self.output3(x[2])  # [20, 20, out_ch]\n\n        up_h, up_w = tf.shape(output2)[1], tf.shape(output2)[2]\n        up3 = tf.image.resize(output3, [up_h, up_w], method='nearest')\n        output2 = output2 + up3\n        output2 = self.merge2(output2)\n\n        up_h, up_w = tf.shape(output1)[1], tf.shape(output1)[2]\n        up2 = tf.image.resize(output2, [up_h, up_w], method='nearest')\n        output1 = output1 + up2\n        output1 = self.merge1(output1)\n\n        return output1, output2, output3\n    \nclass SSH(tf.keras.layers.Layer):\n    \"\"\"Single Stage Headless Layer\"\"\"\n    def __init__(self, out_ch, wd, name='SSH', **kwargs):\n        super(SSH, self).__init__(name=name, **kwargs)\n        assert out_ch % 4 == 0\n        act = 'relu'\n        if (out_ch <= 64):\n            act = 'lrelu'\n\n        self.conv_3x3 = ConvUnit(f=out_ch // 2, k=3, s=1, wd=wd, act=None)\n\n        self.conv_5x5_1 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=act)\n        self.conv_5x5_2 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=None)\n\n        self.conv_7x7_2 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=act)\n        self.conv_7x7_3 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=None)\n\n        self.relu = ReLU()\n              \n    \nclass BboxHead(tf.keras.layers.Layer):\n    \"\"\"Bbox Head Layer\"\"\"\n    def __init__(self, num_anchor, wd, name='BboxHead', **kwargs):\n        super(BboxHead, self).__init__(name=name, **kwargs)\n        self.num_anchor = num_anchor\n        self.conv = Conv2D(filters=num_anchor * 4, kernel_size=1, strides=1)\n\n    def call(self, x):\n        h, w = tf.shape(x)[1], tf.shape(x)[2]\n        x = self.conv(x)\n\n        return tf.reshape(x, [-1, h * w * self.num_anchor, 4])\n\n\nclass LandmarkHead(tf.keras.layers.Layer):\n    \"\"\"Landmark Head Layer\"\"\"\n    def __init__(self, num_anchor, wd, name='LandmarkHead', **kwargs):\n        super(LandmarkHead, self).__init__(name=name, **kwargs)\n        self.num_anchor = num_anchor\n        self.conv = Conv2D(filters=num_anchor * 10, kernel_size=1, strides=1)\n\n    def call(self, x):\n        h, w = tf.shape(x)[1], tf.shape(x)[2]\n        x = self.conv(x)\n\n        return tf.reshape(x, [-1, h * w * self.num_anchor, 10])\n    \ndef Backbone(backbone_type='MobileNetV2', use_pretrain=True):\n    \"\"\"Backbone Model\"\"\"\n    weights = None\n    if use_pretrain:\n        weights = 'imagenet'\n\n    def backbone(x):\n        if backbone_type == 'ResNet50':\n            extractor = ResNet50(\n                input_shape=x.shape[1:], include_top=False, weights=weights)\n            pick_layer1 = 80  # [80, 80, 512]\n            pick_layer2 = 142  # [40, 40, 1024]\n            pick_layer3 = 174  # [20, 20, 2048]\n            preprocess = tf.keras.applications.resnet.preprocess_input\n        elif backbone_type == 'MobileNetV2':\n            extractor = MobileNetV3Small(   #MobileNetV2(\n                input_shape=x.shape[1:], include_top=False, weights=weights)\n            pick_layer1 = 40  # [80, 80, 32] #pick_layer1 = 54  # [80, 80, 32]\n            pick_layer2 = 109 # [40, 40, 96] #pick_layer2 = 116  # [40, 40, 96]\n            pick_layer3 = 200  # [20, 20, 160] #pick_layer3 = 143  # [20, 20, 160]\n            preprocess = tf.keras.applications.mobilenet_v3.preprocess_input #tf.keras.applications.mobilenet_v2.preprocess_input\n        else:\n            raise NotImplementedError(\n                'Backbone type {} is not recognized.'.format(backbone_type))\n\n        return Model(extractor.input,\n                     (extractor.layers[pick_layer1].output,\n                      extractor.layers[pick_layer2].output,\n                      extractor.layers[pick_layer3].output),\n                     name='MobileNetV3Small_extrator')(preprocess(x))\n\n    return backbone\n\nclass ClassHead(tf.keras.layers.Layer):\n    \"\"\"Class Head Layer\"\"\"\n    def __init__(self, num_anchor, wd, name='ClassHead', **kwargs):\n        super(ClassHead, self).__init__(name=name, **kwargs)\n        self.num_anchor = num_anchor\n        self.conv = Conv2D(filters=num_anchor * 2, kernel_size=1, strides=1)\n\n    def call(self, x):\n        h, w = tf.shape(x)[1], tf.shape(x)[2]\n        x = self.conv(x)\n\n        return tf.reshape(x, [-1, h * w * self.num_anchor, 2])\n\ndef RetinaFaceModel(cfg, training=False, iou_th=0.4, score_th=0.02,\n                    name='RetinaFaceModel'):\n    \"\"\"Retina Face Model\"\"\"\n    input_size = cfg['input_size'] if training else None\n    wd = cfg['weights_decay']\n    out_ch = cfg['out_channel']\n    num_anchor = len(cfg['min_sizes'][0])\n    backbone_type = cfg['backbone_type']\n\n    # define model\n    x = inputs = Input([input_size, input_size, 3], name='input_image')\n\n    x = Backbone(backbone_type=backbone_type)(x)\n\n    fpn = FPN(out_ch=out_ch, wd=wd)(x)\n\n    features = [SSH(out_ch=out_ch, wd=wd, name=f'SSH_{i}')(f)\n                for i, f in enumerate(fpn)]\n\n    bbox_regressions = tf.concat(\n        [BboxHead(num_anchor, wd=wd, name=f'BboxHead_{i}')(f)\n         for i, f in enumerate(features)], axis=1)\n    landm_regressions = tf.concat(\n        [LandmarkHead(num_anchor, wd=wd, name=f'LandmarkHead_{i}')(f)\n         for i, f in enumerate(features)], axis=1)\n    classifications = tf.concat(\n        [ClassHead(num_anchor, wd=wd, name=f'ClassHead_{i}')(f)\n         for i, f in enumerate(features)], axis=1)\n\n    classifications = tf.keras.layers.Softmax(axis=-1)(classifications)\n\n    if training:\n        out = (bbox_regressions, landm_regressions, classifications)\n    else:\n        # only for batch size 1\n        preds = tf.concat(  # [bboxes, landms, landms_valid, conf]\n            [bbox_regressions[0], landm_regressions[0],\n             tf.ones_like(classifications[0, :, 0][..., tf.newaxis]),\n             classifications[0, :, 1][..., tf.newaxis]], 1)\n        \n\n        priors = prior_box_tf((tf.shape(inputs)[1], tf.shape(inputs)[2]),\n                                cfg['min_sizes'],  cfg['steps'], cfg['clip'])\n\n        decode_preds = decode_tf(preds, priors, cfg['variances'])\n\n        selected_indices = tf.image.non_max_suppression(\n            boxes=decode_preds[:, :4],\n            scores=decode_preds[:, -1],\n            max_output_size=tf.shape(decode_preds)[0],\n            iou_threshold=iou_th,\n            score_threshold=score_th)\n\n        out = tf.gather(decode_preds, selected_indices)\n\n    return Model(inputs, out, name=name)","metadata":{"_uuid":"7ddcd9ad-fd81-4af3-a51f-8577d85d3c72","_cell_guid":"20a39e08-69b9-4150-904a-74e945707d24","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:47.270962Z","iopub.execute_input":"2022-01-10T10:45:47.271425Z","iopub.status.idle":"2022-01-10T10:45:47.328415Z","shell.execute_reply.started":"2022-01-10T10:45:47.27139Z","shell.execute_reply":"2022-01-10T10:45:47.327412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#функции для работы с датасетом\n\ndef _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,\n                    using_encoding, priors, match_thresh, ignore_thresh,\n                    variances):\n    def parse_tfrecord(tfrecord):\n        features = {\n            'image/img_name': tf.io.FixedLenFeature([], tf.string),\n            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark0/x': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark0/y': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark1/x': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark1/y': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark2/x': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark2/y': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark3/x': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark3/y': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark4/x': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark4/y': tf.io.VarLenFeature(tf.float32),\n            'image/object/landmark/valid': tf.io.VarLenFeature(tf.float32)}\n        if using_bin:\n            features['image/encoded'] = tf.io.FixedLenFeature([], tf.string)\n            x = tf.io.parse_single_example(tfrecord, features)\n            img = tf.image.decode_jpeg(x['image/encoded'], channels=3)\n        else:\n            features['image/img_path'] = tf.io.FixedLenFeature([], tf.string)\n            x = tf.io.parse_single_example(tfrecord, features)\n            image_encoded = tf.io.read_file(x['image/img_path'])\n            img = tf.image.decode_jpeg(image_encoded, channels=3)\n\n        labels = tf.stack(\n            [tf.sparse.to_dense(x['image/object/bbox/xmin']),\n             tf.sparse.to_dense(x['image/object/bbox/ymin']),\n             tf.sparse.to_dense(x['image/object/bbox/xmax']),\n             tf.sparse.to_dense(x['image/object/bbox/ymax']),\n             tf.sparse.to_dense(x['image/object/landmark0/x']),\n             tf.sparse.to_dense(x['image/object/landmark0/y']),\n             tf.sparse.to_dense(x['image/object/landmark1/x']),\n             tf.sparse.to_dense(x['image/object/landmark1/y']),\n             tf.sparse.to_dense(x['image/object/landmark2/x']),\n             tf.sparse.to_dense(x['image/object/landmark2/y']),\n             tf.sparse.to_dense(x['image/object/landmark3/x']),\n             tf.sparse.to_dense(x['image/object/landmark3/y']),\n             tf.sparse.to_dense(x['image/object/landmark4/x']),\n             tf.sparse.to_dense(x['image/object/landmark4/y']),\n             tf.sparse.to_dense(x['image/object/landmark/valid'])], axis=1)\n\n        img, labels = _transform_data(\n            img_dim, using_flip, using_distort, using_encoding, priors,\n            match_thresh, ignore_thresh, variances)(img, labels)\n\n        return img, labels\n    return parse_tfrecord\n\n\ndef _transform_data(img_dim, using_flip, using_distort, using_encoding, priors,\n                    match_thresh, ignore_thresh, variances):\n    def transform_data(img, labels):\n        img = tf.cast(img, tf.float32)\n\n        # randomly crop\n        img, labels = _crop(img, labels)\n\n        # padding to square\n        img = _pad_to_square(img)\n\n        # resize\n        img, labels = _resize(img, labels, img_dim)\n\n        # randomly left-right flip\n        if using_flip:\n            img, labels = _flip(img, labels)\n\n        # distort\n        if using_distort:\n            img = _distort(img)\n\n        # encode labels to feature targets\n        if using_encoding:\n            labels = encode_tf(labels=labels, priors=priors,\n                               match_thresh=match_thresh,\n                               ignore_thresh=ignore_thresh,\n                               variances=variances)\n\n        return img, labels\n    return transform_data\n\n\ndef load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n                          using_bin=True, using_flip=True, using_distort=True,\n                          using_encoding=True, priors=None, match_thresh=0.45,\n                          ignore_thresh=0.3, variances=[0.1, 0.2],\n                          shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    if not using_encoding:\n        assert batch_size == 1  # dynamic data len when using_encoding\n    else:\n        assert priors is not None\n\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,\n                        using_encoding, priors, match_thresh, ignore_thresh,\n                        variances),\n        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(\n        buffer_size=tf.data.experimental.AUTOTUNE)\n\n    return dataset\n\n\n###############################################################################\n#   Data Augmentation                                                         #\n###############################################################################\ndef _flip(img, labels):\n    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n\n    def flip_func():\n        flip_img = tf.image.flip_left_right(img)\n        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n                                1 - labels[:, 0],  labels[:, 3],\n                                1 - labels[:, 6],  labels[:, 7],\n                                1 - labels[:, 4],  labels[:, 5],\n                                1 - labels[:, 8],  labels[:, 9],\n                                1 - labels[:, 12], labels[:, 13],\n                                1 - labels[:, 10], labels[:, 11],\n                                labels[:, 14]], axis=1)\n\n        return flip_img, flip_labels\n\n    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],\n                          default=lambda: (img, labels))\n\n    return img, labels\n\n\ndef _crop(img, labels, max_loop=250):\n    shape = tf.shape(img)\n\n    def matrix_iof(a, b):\n        \"\"\"\n        return iof of a and b, numpy version for data augenmentation\n        \"\"\"\n        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n\n        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n\n    def crop_loop_body(i, img, labels):\n        valid_crop = tf.constant(1, tf.int32)\n\n        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n        h = w = tf.cast(scale * short_side, tf.int32)\n        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n        roi = tf.cast(roi, tf.float32)\n\n        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n                             lambda: valid_crop, lambda: 0)\n\n        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n        mask_a = tf.reduce_all(\n            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n            axis=1)\n        labels_t = tf.boolean_mask(labels, mask_a)\n        valid_crop = tf.cond(tf.reduce_any(mask_a),\n                             lambda: valid_crop, lambda: 0)\n\n        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n        h_offset = tf.cast(h_offset, tf.float32)\n        w_offset = tf.cast(w_offset, tf.float32)\n        labels_t = tf.stack(\n            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n             labels_t[:, 4] - w_offset,  labels_t[:, 5] - h_offset,\n             labels_t[:, 6] - w_offset,  labels_t[:, 7] - h_offset,\n             labels_t[:, 8] - w_offset,  labels_t[:, 9] - h_offset,\n             labels_t[:, 10] - w_offset, labels_t[:, 11] - h_offset,\n             labels_t[:, 12] - w_offset, labels_t[:, 13] - h_offset,\n             labels_t[:, 14]], axis=1)\n\n        return tf.cond(valid_crop == 1,\n                       lambda: (max_loop, img_t, labels_t),\n                       lambda: (i + 1, img, labels))\n\n    _, img, labels = tf.while_loop(\n        lambda i, img, labels: tf.less(i, max_loop),\n        crop_loop_body,\n        [tf.constant(-1), img, labels],\n        shape_invariants=[tf.TensorShape([]),\n                          tf.TensorShape([None, None, 3]),\n                          tf.TensorShape([None, 15])])\n\n    return img, labels\n\n\ndef _pad_to_square(img):\n    height = tf.shape(img)[0]\n    width = tf.shape(img)[1]\n\n    def pad_h():\n        img_pad_h = tf.ones([width - height, width, 3]) * \\\n            tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n        return tf.concat([img, img_pad_h], axis=0)\n\n    def pad_w():\n        img_pad_w = tf.ones([height, height - width, 3]) * \\\n            tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n        return tf.concat([img, img_pad_w], axis=1)\n\n    img = tf.case([(tf.greater(height, width), pad_w),\n                   (tf.less(height, width), pad_h)], default=lambda: img)\n\n    return img\n\n\ndef _resize(img, labels, img_dim):\n    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n                     labels[:, 2] / w_f,  labels[:, 3] / h_f,\n                     labels[:, 4] / w_f,  labels[:, 5] / h_f,\n                     labels[:, 6] / w_f,  labels[:, 7] / h_f,\n                     labels[:, 8] / w_f,  labels[:, 9] / h_f,\n                     labels[:, 10] / w_f, labels[:, 11] / h_f,\n                     labels[:, 12] / w_f, labels[:, 13] / h_f], axis=1)\n    locs = tf.clip_by_value(locs, 0, 1)\n    labels = tf.concat([locs, labels[:, 14][:, tf.newaxis]], axis=1)\n\n    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n\n    def resize(method):\n        def _resize():\n            return tf.image.resize(\n                img, [img_dim, img_dim], method=method, antialias=True)\n        return _resize\n\n    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n                   (tf.equal(resize_case, 1), resize('area')),\n                   (tf.equal(resize_case, 2), resize('nearest')),\n                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n                  default=resize('bilinear'))\n\n    return img, labels\n\n\ndef _distort(img):\n    img = tf.image.random_brightness(img, 0.4)\n    img = tf.image.random_contrast(img, 0.5, 1.5)\n    img = tf.image.random_saturation(img, 0.5, 1.5)\n    img = tf.image.random_hue(img, 0.1)\n\n    return img","metadata":{"_uuid":"19f4ddbb-5d83-4f92-a189-06bf3ae625be","_cell_guid":"fb1144fb-5104-42fd-8d75-f0d216a44b18","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:47.706975Z","iopub.execute_input":"2022-01-10T10:45:47.707572Z","iopub.status.idle":"2022-01-10T10:45:47.773366Z","shell.execute_reply.started":"2022-01-10T10:45:47.70753Z","shell.execute_reply":"2022-01-10T10:45:47.772279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#вспомогательные функции\n\ndef load_yaml(load_path): \n    \"\"\"load yaml file\"\"\"\n    with open(load_path, 'r') as f:\n        loaded = yaml.load(f, Loader=yaml.Loader)\n\n    return loaded\n\n\ndef set_memory_growth(): #\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            # Currently, memory growth needs to be the same across GPUs\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                logical_gpus = tf.config.experimental.list_logical_devices(\n                    'GPU')\n                logging.info(\n                    \"Detect {} Physical GPUs, {} Logical GPUs.\".format(\n                        len(gpus), len(logical_gpus)))\n        except RuntimeError as e:\n            # Memory growth must be set before GPUs have been initialized\n            logging.info(e)\n\n\ndef load_dataset(cfg, priors, shuffle=True, buffer_size=10240): #\n    \"\"\"load dataset\"\"\"\n    logging.info(\"load dataset from {}\".format(cfg['dataset_path']))\n    dataset = load_tfrecord_dataset(\n        tfrecord_name=cfg['dataset_path'],\n        batch_size=cfg['batch_size'],\n        img_dim=cfg['input_size'],\n        using_bin=cfg['using_bin'],\n        using_flip=cfg['using_flip'],\n        using_distort=cfg['using_distort'],\n        using_encoding=True,\n        priors=priors,\n        match_thresh=cfg['match_thresh'],\n        ignore_thresh=cfg['ignore_thresh'],\n        variances=cfg['variances'],\n        shuffle=shuffle,\n        buffer_size=buffer_size)\n    return dataset\n\n\nclass ProgressBar(object): #\n    \"\"\"A progress bar which can print the progress modified from\n       https://github.com/hellock/cvbase/blob/master/cvbase/progress.py\"\"\"\n    def __init__(self, task_num=0, completed=0, bar_width=25):\n        self.task_num = task_num\n        max_bar_width = self._get_max_bar_width()\n        self.bar_width = (bar_width\n                          if bar_width <= max_bar_width else max_bar_width)\n        self.completed = completed\n        self.first_step = completed\n        self.warm_up = False\n\n    def _get_max_bar_width(self):\n        if sys.version_info > (3, 3):\n            from shutil import get_terminal_size\n        else:\n            from backports.shutil_get_terminal_size import get_terminal_size\n        terminal_width, _ = get_terminal_size()\n        max_bar_width = min(int(terminal_width * 0.6), terminal_width - 50)\n        if max_bar_width < 10:\n            logging.info('terminal width is too small ({}), please consider '\n                         'widen the terminal for better progressbar '\n                         'visualization'.format(terminal_width))\n            max_bar_width = 10\n        return max_bar_width\n\n    def reset(self):\n        \"\"\"reset\"\"\"\n        self.completed = 0\n        self.fps = 0\n\n    def update(self, inf_str=''):\n        \"\"\"update\"\"\"\n        self.completed += 1\n\n        if not self.warm_up:\n            self.start_time = time.time() - 1e-1\n            self.warm_up = True\n\n        if self.completed > self.task_num:\n            self.completed = self.completed % self.task_num\n            self.start_time = time.time() - 1 / self.fps\n            self.first_step = self.completed - 1\n            sys.stdout.write('\\n')\n\n        elapsed = time.time() - self.start_time\n        self.fps = (self.completed - self.first_step) / elapsed\n        percentage = self.completed / float(self.task_num)\n        mark_width = int(self.bar_width * percentage)\n        bar_chars = '>' * mark_width + ' ' * (self.bar_width - mark_width)\n        stdout_str = '\\rTraining [{}] {}/{}, {}  {:.1f} step/sec'\n        sys.stdout.write(stdout_str.format(\n            bar_chars, self.completed, self.task_num, inf_str, self.fps))\n\n        sys.stdout.flush()\n\n\n###############################################################################\n#   Testing                                                                   #\n###############################################################################\ndef pad_input_image(img, max_steps): \n    \"\"\"pad image to suitable shape\"\"\"\n    img_h, img_w, _ = img.shape\n\n    img_pad_h = 0\n    if img_h % max_steps > 0:\n        img_pad_h = max_steps - img_h % max_steps\n\n    img_pad_w = 0\n    if img_w % max_steps > 0:\n        img_pad_w = max_steps - img_w % max_steps\n\n    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n\n    return img, pad_params\n\n\ndef recover_pad_output(outputs, pad_params): \n    \"\"\"recover the padded output effect\"\"\"\n    img_h, img_w, img_pad_h, img_pad_w = pad_params\n    recover_xy = np.reshape(outputs[:, :14], [-1, 7, 2]) * \\\n        [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n    outputs[:, :14] = np.reshape(recover_xy, [-1, 14])\n\n    return outputs\n\n\n###############################################################################\n#   Visulization                                                              #\n###############################################################################\ndef draw_bbox_landm(img, ann, img_height, img_width): \n    \"\"\"draw bboxes and landmarks\"\"\"\n    # bbox\n    x1, y1, x2, y2 = int(ann[0] * img_width), int(ann[1] * img_height), \\\n                     int(ann[2] * img_width), int(ann[3] * img_height)\n    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # confidence\n    text = \"{:.4f}\".format(ann[15])\n    cv2.putText(img, text, (int(ann[0] * img_width), int(ann[1] * img_height)),\n                cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n\n    # landmark\n    if ann[14] > 0:\n        cv2.circle(img, (int(ann[4] * img_width),\n                         int(ann[5] * img_height)), 1, (255, 255, 0), 2) # right eye\n        cv2.circle(img, (int(ann[6] * img_width),\n                         int(ann[7] * img_height)), 1, (0, 255, 255), 2) # left eye\n        cv2.circle(img, (int(ann[8] * img_width),\n                         int(ann[9] * img_height)), 1, (255, 0, 0), 2) # nose\n        cv2.circle(img, (int(ann[10] * img_width),\n                         int(ann[11] * img_height)), 1, (0, 100, 255), 2) # mr\n        cv2.circle(img, (int(ann[12] * img_width),\n                         int(ann[13] * img_height)), 1, (255, 0, 100), 2) # ml","metadata":{"_uuid":"4719463b-c049-4458-90f1-aca953b4aeaf","_cell_guid":"67c2f80d-97fb-4d1b-896d-bd411b509c9b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:48.010777Z","iopub.execute_input":"2022-01-10T10:45:48.011066Z","iopub.status.idle":"2022-01-10T10:45:48.041283Z","shell.execute_reply.started":"2022-01-10T10:45:48.011032Z","shell.execute_reply":"2022-01-10T10:45:48.040436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"1e3b4bb2-e2bf-4938-8aeb-6be23037ee72","_cell_guid":"876c19e0-fb0a-4ee0-8be9-810b74e83488","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Структура содели\nclass RetinaFace:\n    \n    # Загрузка чекпойнта\n    def __load_checkpoint(self):\n\n        checkpoint_dir = './checkpoints/' + self.cfg['sub_name']\n        checkpoint = tf.train.Checkpoint(step=tf.Variable(0, name='step'),\n                                         optimizer=self.optimizer,\n                                         model=self.model)\n        manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n                                             directory=checkpoint_dir,\n                                             max_to_keep=3)\n        '''\n        if manager.latest_checkpoint:\n            checkpoint.restore(manager.latest_checkpoint)\n            print('[*] load ckpt from {} at step {} for training.'.format(\n                manager.latest_checkpoint, checkpoint.step.numpy()))\n        else:\n            print(\"[*] training from scratch.\")\n            '''\n            \n        return manager, checkpoint\n    \n\n        \n    \n    def __init__(self, cfg, **kwargs):\n        # init\n        SCORE_TH = 0.5\n        IOU_TH = 0.4\n        \n        self.manager = None\n        self.checkpoint = None\n        \n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n        logger = tf.get_logger()\n        logger.disabled = True\n        logger.setLevel(logging.FATAL)\n        set_memory_growth()\n        \n        # Конфиг\n        self.cfg = load_yaml(cfg)\n        \n        # Наша модель\n        self.model = RetinaFaceModel(self.cfg, training=True)\n        self.predict_model = RetinaFaceModel(self.cfg, training=False, iou_th=IOU_TH,\n                            score_th=SCORE_TH)\n        \n        self.priors = prior_box((self.cfg['input_size'], self.cfg['input_size']),\n                           self.cfg['min_sizes'],  self.cfg['steps'], self.cfg['clip'])\n        \n        # Датасет\n        self.train_dataset = load_dataset(self.cfg, self.priors, shuffle=True)\n        \n        # Шаг\n        self.steps_per_epoch = self.cfg['dataset_len'] // self.cfg['batch_size']\n        \n        # LR\n        self.learning_rate = MultiStepWarmUpLR(\n            initial_learning_rate=self.cfg['init_lr'],\n            lr_steps=[e * self.steps_per_epoch for e in self.cfg['lr_decay_epoch']],\n            lr_rate=self.cfg['lr_rate'],\n            warmup_steps=self.cfg['warmup_epoch'] * self.steps_per_epoch,\n            min_lr=self.cfg['min_lr'])\n        \n        # Otpimizer (SGD)\n        self.optimizer = tf.keras.optimizers.SGD(\n            learning_rate=self.learning_rate, momentum=0.9, nesterov=True)\n        \n        # Loss\n        self.multi_box_loss = MultiBoxLoss()\n        \n        # Fit Status\n        self.is_fitted = False\n  \n    \n    def get_model(self):\n        return self.model\n    \n    # Обучаем модель\n    def fit(self):\n        # Чекпойнты\n        self.manager, self.checkpoint = self.__load_checkpoint()    \n        \n        # Шаг обучения\n        @tf.function\n        def train_step(inputs, labels):\n            with tf.GradientTape() as tape:\n                predictions = self.model(inputs, training=True)\n\n                losses = {}\n                losses['reg'] = tf.reduce_sum(self.model.losses)\n                losses['loc'], losses['landm'], losses['class'] = \\\n                    self.multi_box_loss(labels, predictions)\n                total_loss = tf.add_n([l for l in losses.values()])\n\n            grads = tape.gradient(total_loss, self.model.trainable_variables)\n            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n\n            return total_loss, losses\n\n        # Цикл обучения модели\n        summary_writer = tf.summary.create_file_writer('./logs/' + self.cfg['sub_name'])\n        remain_steps = max(\n            self.steps_per_epoch * self.cfg['epoch'] - self.checkpoint.step.numpy(), 0)\n        prog_bar = ProgressBar(self.steps_per_epoch,\n                               self.checkpoint.step.numpy() % self.steps_per_epoch)\n        \n        if remain_steps > 0:\n            for inputs, labels in self.train_dataset.take(remain_steps):\n                self.checkpoint.step.assign_add(1)\n                steps = self.checkpoint.step.numpy()\n\n                total_loss, losses = train_step(inputs, labels)\n\n                prog_bar.update(\"epoch={}/{}, loss={:.4f}, lr={:.1e}\".format(\n                    ((steps - 1) // self.steps_per_epoch) + 1, self.cfg['epoch'],\n                    total_loss.numpy(), self.optimizer.lr(steps).numpy()))\n\n                if steps % 10 == 0:\n                    with summary_writer.as_default():\n                        tf.summary.scalar(\n                            'loss/total_loss', total_loss, step=steps)\n                        for k, l in losses.items():\n                            tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n                        tf.summary.scalar(\n                            'learning_rate', self.optimizer.lr(steps), step=steps)\n\n                if steps % self.cfg['save_steps'] == 0:\n                    self.manager.save()\n                    print(\"\\n[*] save ckpt file at {}\".format(\n                        self.manager.latest_checkpoint))\n\n            self.manager.save()\n            print(\"\\n[*] training done! save ckpt file at {}\".format(\n                self.manager.latest_checkpoint))\n        \n        self.is_fitted = True\n    \n    \n    '''Принимает список изображений'''\n    def predict(self, X):\n        down_scale_factor = 1\n        \n        assert self.is_fitted == True, \"Сначала обучите модель методом fit\"\n        assert isinstance(X, list), \"Передайте список изображений numpy\"\n        \n        # Inference model checkpoint\n        checkpoint_dir = './checkpoints/' + self.cfg['sub_name']\n        checkpoint = tf.train.Checkpoint(model=self.predict_model)\n        \n        if tf.train.latest_checkpoint(checkpoint_dir):\n            checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n            print(\"[*] load ckpt from {} to inference model.\".format(\n                tf.train.latest_checkpoint(checkpoint_dir)))\n        else:\n            print(\"[*] Cannot find ckpt from {}.\".format(checkpoint_dir))\n            exit()\n        \n        predicts = {}\n        \n        predicts['outputs'] = []\n        predicts['image_params'] = []\n        \n        for _img in X:\n            \n            img_raw = cv2.imread(_img)\n            img_height_raw, img_width_raw, _ = img_raw.shape\n            img = np.float32(img_raw.copy())\n\n            if down_scale_factor < 1.0:\n                img = cv2.resize(img, (0, 0), fx=down_scale_factor,\n                                 fy=down_scale_factor,\n                                 interpolation=cv2.INTER_LINEAR)\n                \n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # pad input image to avoid unmatched shape problem\n            img, pad_params = pad_input_image(img, max_steps=max(self.cfg['steps']))\n\n            # run model\n            outputs = self.predict_model(img[np.newaxis, ...]).numpy()\n\n            # recover padding effect\n            outputs = recover_pad_output(outputs, pad_params)\n            \n            predicts['outputs'].append(outputs)\n            predicts['image_params'].append((img_width_raw, img_height_raw))\n        \n        \n        return predicts","metadata":{"_uuid":"d616b238-3b28-4814-a5fa-5b722197f0f6","_cell_guid":"45e2a320-b3b9-4e93-95de-494ab784a50d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:48.46279Z","iopub.execute_input":"2022-01-10T10:45:48.463393Z","iopub.status.idle":"2022-01-10T10:45:48.495455Z","shell.execute_reply.started":"2022-01-10T10:45:48.463356Z","shell.execute_reply":"2022-01-10T10:45:48.494642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"1fc568a3-3016-43c9-832c-bfae27a8f97d","_cell_guid":"fdb54cc5-d254-4548-ab68-a67c5fa41358","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Зададим настройки обучения модели\nBATCH_SIZE = 12 \nEPOCHS = 60\n#MobileNetV2\n## Настройки конфигов обучения\nMODELS_CONFIG = {\n    'MobileNetV2': {\n        'cfg_path': f'../input/configresnet/retinaface_res50.yaml',\n        'dataset_path': f'../input/widerface/widerface_train_bin.tfrecord',\n        'batch_size': BATCH_SIZE,\n        'testing_dataset_path': f'../input/widerface/val',\n        'epoch': EPOCHS\n    }\n}\n\n# Выберем backbone для RetinaFace\nbackbone_model = 'MobileNetV2'\n\ndataset_path = MODELS_CONFIG[backbone_model]['dataset_path']\ncfg_path = MODELS_CONFIG[backbone_model]['cfg_path']\nbatch_size = MODELS_CONFIG[backbone_model]['batch_size'] \ntesting_dataset_path = MODELS_CONFIG[backbone_model]['testing_dataset_path']\nepoch = MODELS_CONFIG[backbone_model]['epoch']\n\ncustom_config_path = f'./retinaface_res50.yaml'","metadata":{"_uuid":"a6703b0f-6f7a-40bd-99b0-6a3afcbcf913","_cell_guid":"83724f8f-4ea9-4d79-9bee-6c2de493666c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:48.941914Z","iopub.execute_input":"2022-01-10T10:45:48.942197Z","iopub.status.idle":"2022-01-10T10:45:48.948677Z","shell.execute_reply.started":"2022-01-10T10:45:48.942164Z","shell.execute_reply":"2022-01-10T10:45:48.947638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"852bb9a1-6c4c-4010-a881-2c03372d6d9c","_cell_guid":"fb7a00c1-82d8-4ffb-a3ac-04933f31affa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Внесем в конфиг локальные пути\nimport re\n\nprint('Вносим изменения в конфигурационный файл...')\n\nwith open('../input/configresnet/retinaface_res50.yaml') as f:\n    s = f.read()\nwith open(custom_config_path, 'w') as f:\n\n    # Batch_size\n    s = re.sub('batch_size: [0-9]+',\n               'batch_size: {}'.format(batch_size), s)\n\n    # Epochs\n    epoch_re = re.compile(r\"^epoch: [0-9]+\", re.MULTILINE)\n    s = re.sub(epoch_re,\n               'epoch: {}'.format(epoch), s)\n    \n    # Dataset path\n    s = re.sub('dataset_path: \\'.\\/data\\/widerface_train_bin.tfrecord\\'', f\"dataset_path: '{dataset_path}'\", s)\n    \n    s = re.sub('backbone_type: \\'ResNet50\\'', 'backbone_type: \\'MobileNetV2\\'', s)\n    \n    # Val path\n    s = re.sub('testing_dataset_path: \\'.\\/data\\/widerface\\/val\\'', f\"testing_dataset_path: '{testing_dataset_path}'\", s)\n            \n    f.write(s)\n    \nprint(f'Создан новый конфигурационный файл {custom_config_path}')","metadata":{"_uuid":"386384a4-6a71-4d9f-9483-892633681390","_cell_guid":"f2ec0a07-b495-431b-83af-4bd1d2025163","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:49.410089Z","iopub.execute_input":"2022-01-10T10:45:49.410379Z","iopub.status.idle":"2022-01-10T10:45:49.421592Z","shell.execute_reply.started":"2022-01-10T10:45:49.410345Z","shell.execute_reply":"2022-01-10T10:45:49.420773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat {custom_config_path}","metadata":{"_uuid":"49a8607c-3ab1-4f1d-b9cb-932dd1630c9c","_cell_guid":"e9d923fc-c94d-4251-9da5-c7b4e9733308","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:49.63824Z","iopub.execute_input":"2022-01-10T10:45:49.6388Z","iopub.status.idle":"2022-01-10T10:45:50.33716Z","shell.execute_reply.started":"2022-01-10T10:45:49.638761Z","shell.execute_reply":"2022-01-10T10:45:50.336178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RetinaFace(custom_config_path)","metadata":{"_uuid":"5f4354ef-8cf1-4657-9852-eca296719b7b","_cell_guid":"e1f130dd-9e16-4926-8d54-f2d651e31634","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:50.526504Z","iopub.execute_input":"2022-01-10T10:45:50.526808Z","iopub.status.idle":"2022-01-10T10:45:58.282834Z","shell.execute_reply.started":"2022-01-10T10:45:50.526774Z","shell.execute_reply":"2022-01-10T10:45:58.282084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_model().summary()","metadata":{"_uuid":"4044e3aa-e511-4691-a2af-615a211eced3","_cell_guid":"1c127813-f494-4562-8c68-af8c70cf9da9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:58.284703Z","iopub.execute_input":"2022-01-10T10:45:58.284968Z","iopub.status.idle":"2022-01-10T10:45:58.311844Z","shell.execute_reply.started":"2022-01-10T10:45:58.284931Z","shell.execute_reply":"2022-01-10T10:45:58.311158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape          = (640, 640, 3)\nmobil_net2 = base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape = input_shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T10:45:25.85013Z","iopub.execute_input":"2022-01-10T10:45:25.850395Z","iopub.status.idle":"2022-01-10T10:45:26.893728Z","shell.execute_reply.started":"2022-01-10T10:45:25.85035Z","shell.execute_reply":"2022-01-10T10:45:26.892936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mobil_net2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T10:45:26.895259Z","iopub.execute_input":"2022-01-10T10:45:26.895521Z","iopub.status.idle":"2022-01-10T10:45:26.900803Z","shell.execute_reply.started":"2022-01-10T10:45:26.895486Z","shell.execute_reply":"2022-01-10T10:45:26.899099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Обучим нашу модель или загрузим последний чекпойнт\nmodel.fit()","metadata":{"_uuid":"e42fd8ad-0d05-4eaa-9a74-cea92127c116","_cell_guid":"e9b5a69f-c2e4-4750-a93f-da353957c961","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-10T10:45:26.902302Z","iopub.execute_input":"2022-01-10T10:45:26.902571Z","iopub.status.idle":"2022-01-10T10:45:41.825928Z","shell.execute_reply.started":"2022-01-10T10:45:26.902534Z","shell.execute_reply":"2022-01-10T10:45:41.823628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Посмотрим на выходные данные\n!ls -la './checkpoints/retinaface_res50/'","metadata":{"_uuid":"abe1c9db-1555-4fdb-8ed5-18b0bb18f9e5","_cell_guid":"884d808e-a4df-456e-be5d-ebfb0ecc845c","execution":{"iopub.status.busy":"2022-01-04T21:11:54.767412Z","iopub.execute_input":"2022-01-04T21:11:54.768079Z","iopub.status.idle":"2022-01-04T21:11:55.49818Z","shell.execute_reply.started":"2022-01-04T21:11:54.768039Z","shell.execute_reply":"2022-01-04T21:11:55.497351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./checkpoints/retinaface_res50/checkpoint')","metadata":{"_uuid":"ba37ca9f-537e-46ab-8599-4adcd0a44a62","_cell_guid":"74a99721-68c2-4f90-bad0-b41aac11d434","execution":{"iopub.status.busy":"2022-01-04T21:13:05.805518Z","iopub.execute_input":"2022-01-04T21:13:05.805775Z","iopub.status.idle":"2022-01-04T21:13:05.810764Z","shell.execute_reply.started":"2022-01-04T21:13:05.805746Z","shell.execute_reply":"2022-01-04T21:13:05.810113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Протестируем нашу модель на валидационной выборке\ntest_images = []\n\nfor path, subdirs, files in os.walk('../input/widerface/val'):\n    for name in files:\n        if name.endswith('.jpg'):\n            test_images.append(os.path.join(path, name))","metadata":{"_uuid":"27e24397-80ac-4ab0-bf4b-970793ab685e","_cell_guid":"a1964099-e1b4-44e5-b15c-c2cc0804fdf6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-04T21:13:23.222158Z","iopub.execute_input":"2022-01-04T21:13:23.222418Z","iopub.status.idle":"2022-01-04T21:13:24.715541Z","shell.execute_reply.started":"2022-01-04T21:13:23.222388Z","shell.execute_reply":"2022-01-04T21:13:24.714613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = ['../input/widerface/val/images/12--Group/12_Group_Group_12_Group_Group_12_367.jpg']","metadata":{"execution":{"iopub.status.busy":"2022-01-04T21:14:18.234072Z","iopub.execute_input":"2022-01-04T21:14:18.234606Z","iopub.status.idle":"2022-01-04T21:14:18.237931Z","shell.execute_reply.started":"2022-01-04T21:14:18.234567Z","shell.execute_reply":"2022-01-04T21:14:18.237254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#y_pred = model.predict(test_images[:1])\nmodel.is_fitted = True","metadata":{"_uuid":"55a7aafb-3969-47b2-9930-ca4ec784f57d","_cell_guid":"5fa83efe-8d43-4fe7-93fe-ac78f355f0c0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-03T04:05:23.887164Z","iopub.execute_input":"2022-01-03T04:05:23.887427Z","iopub.status.idle":"2022-01-03T04:05:23.890928Z","shell.execute_reply.started":"2022-01-03T04:05:23.887399Z","shell.execute_reply":"2022-01-03T04:05:23.89016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_images)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T21:14:22.629449Z","iopub.execute_input":"2022-01-04T21:14:22.630165Z","iopub.status.idle":"2022-01-04T21:14:23.637021Z","shell.execute_reply.started":"2022-01-04T21:14:22.630124Z","shell.execute_reply":"2022-01-04T21:14:23.636292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_images[:1]","metadata":{"_uuid":"dc72e763-3c30-49ef-a6ed-6c6c88310688","_cell_guid":"56d68b13-9629-416f-b398-08bc18238776","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-20T14:28:27.713332Z","iopub.execute_input":"2021-12-20T14:28:27.713587Z","iopub.status.idle":"2021-12-20T14:28:27.720097Z","shell.execute_reply.started":"2021-12-20T14:28:27.713556Z","shell.execute_reply":"2021-12-20T14:28:27.719425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция рисования рамки и ключевых точек\ndef draw_bbox_and_landmarks(raw_image_list, predictions):\n    \n    preds_imgs = []\n    \n    for idx, pred in enumerate(predictions):\n        img_raw = cv2.imread(raw_image_list[idx])\n        img_height_raw, img_width_raw, _ = img_raw.shape\n               \n        for prior_index in range(len(pred)):\n            draw_bbox_landm(img_raw, pred[prior_index], img_height_raw,\n                            img_width_raw)\n            \n        preds_imgs.append(img_raw)\n        \n    return preds_imgs\n\n\ndef decode_predictions(image_params, y_pred_outputs):\n    \n    face_dict = {}\n\n    for pred_index in range(len(y_pred_outputs)):\n        img_width, img_height = image_params[0], image_params[1]\n        ann = y_pred_outputs[pred_index]\n\n        # bbox\n        x1, y1, x2, y2 = int(ann[0] * img_width), int(ann[1] * img_height), \\\n                         int(ann[2] * img_width), int(ann[3] * img_height)\n\n        # confidence\n        conf = \"{:.4f}\".format(ann[15])\n\n        # landmark\n        if ann[14] > 0:\n            right_eye = (int(ann[4] * img_width), int(ann[5] * img_height))\n            left_eye = (int(ann[6] * img_width), int(ann[7] * img_height))\n            nose = (int(ann[8] * img_width), int(ann[9] * img_height))\n            mouth_right = (int(ann[10] * img_width), int(ann[11] * img_height))\n            mouth_left = (int(ann[12] * img_width),int(ann[13] * img_height))           \n\n        current_face = {'bbox': [x1, y1, x2, y2], 'score':conf, 'right_eye': right_eye, \\\n                        'left_eye': left_eye, 'nose': nose, 'mouth_right': mouth_right, \\\n                        'mouth_left': mouth_left}\n\n        face_dict.setdefault(f'face_{pred_index}', []).append(current_face)\n\n    return face_dict","metadata":{"_uuid":"27351cd2-2e85-4b68-b63a-78964406f57e","_cell_guid":"db6d9363-32eb-4038-8e22-3c65da73ec42","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-04T21:13:37.561346Z","iopub.execute_input":"2022-01-04T21:13:37.561794Z","iopub.status.idle":"2022-01-04T21:13:37.57523Z","shell.execute_reply.started":"2022-01-04T21:13:37.561755Z","shell.execute_reply":"2022-01-04T21:13:37.57441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c5b6d667-b73c-4016-8867-5bcefde9a3c6","_cell_guid":"29f3580e-469b-4fd5-86c9-1f7eb7ddb4d7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Нарисуем баундбоксы\nboxed_img = draw_bbox_and_landmarks(test_images, y_pred['outputs'])\nplt.figure(figsize=(20,10))\nplt.imshow(cv2.cvtColor(boxed_img[0], cv2.COLOR_BGR2RGB))","metadata":{"_uuid":"31365f98-7387-49db-bdf0-394330051b4c","_cell_guid":"561ab9e2-0e2c-4442-8022-e63cf7ee0a00","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-04T21:14:28.369235Z","iopub.execute_input":"2022-01-04T21:14:28.369916Z","iopub.status.idle":"2022-01-04T21:14:29.031653Z","shell.execute_reply.started":"2022-01-04T21:14:28.36988Z","shell.execute_reply":"2022-01-04T21:14:29.030286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom IPython.display import Image\nImage(filename=\"../input/widerface/val/images/12--Group/12_Group_Group_12_Group_Group_12_10.jpg\", width=320, height=240)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T19:05:24.869774Z","iopub.execute_input":"2021-12-20T19:05:24.870522Z","iopub.status.idle":"2021-12-20T19:05:24.886105Z","shell.execute_reply.started":"2021-12-20T19:05:24.870477Z","shell.execute_reply":"2021-12-20T19:05:24.885461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"../input/widerface/val/images/12--Group/12_Group_Group_12_Group_Group_12_10.jpg\", width=320, heigth=240>\n\n![jupyter](../input/widerface/val/images/12--Group/12_Group_Group_12_Group_Group_12_10.jpg)","metadata":{}},{"cell_type":"code","source":"model.get_model().summary()","metadata":{"_uuid":"66959b8b-87cf-444b-98c4-7c49c3f38b1f","_cell_guid":"153c76c9-1de0-40e6-94d4-912357e48f27","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-20T15:01:21.079708Z","iopub.execute_input":"2021-12-20T15:01:21.080443Z","iopub.status.idle":"2021-12-20T15:01:21.144408Z","shell.execute_reply.started":"2021-12-20T15:01:21.080347Z","shell.execute_reply":"2021-12-20T15:01:21.143474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_model().save('my_retina1')","metadata":{"_uuid":"19b36ce5-57dd-45a3-b980-b30dceecd9ff","_cell_guid":"88de8178-0ded-46ec-b643-ef23728ff731","collapsed":false,"execution":{"iopub.status.busy":"2021-12-20T13:45:53.248513Z","iopub.execute_input":"2021-12-20T13:45:53.249329Z","iopub.status.idle":"2021-12-20T13:46:26.986935Z","shell.execute_reply.started":"2021-12-20T13:45:53.249281Z","shell.execute_reply":"2021-12-20T13:46:26.986166Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la './my_retina1/variables'","metadata":{"_uuid":"2598b555-c955-4de0-8b4c-e2a0842a5c9c","_cell_guid":"bab29caf-98fe-49c9-ad76-96acb8943086","collapsed":false,"execution":{"iopub.status.busy":"2021-12-20T13:49:11.445201Z","iopub.execute_input":"2021-12-20T13:49:11.446071Z","iopub.status.idle":"2021-12-20T13:49:12.340277Z","shell.execute_reply.started":"2021-12-20T13:49:11.446036Z","shell.execute_reply":"2021-12-20T13:49:12.339436Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'./my_retina1/variables/variables.data-00000-of-00001')","metadata":{"_uuid":"b88dc50e-5f34-4eb3-9471-fadcc0b2200a","_cell_guid":"d8a143ce-dd0b-44d5-8b1e-72e5cf3630bc","collapsed":false,"execution":{"iopub.status.busy":"2021-12-20T13:49:49.245282Z","iopub.execute_input":"2021-12-20T13:49:49.245824Z","iopub.status.idle":"2021-12-20T13:49:49.25312Z","shell.execute_reply.started":"2021-12-20T13:49:49.245784Z","shell.execute_reply":"2021-12-20T13:49:49.252319Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"5853378b-fd4c-4eac-960b-da0c8a761763","_cell_guid":"e75f925b-4050-42bd-b61c-d5ce5d734ff0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"558187ab-9c68-429a-b3d6-a4ddd3cc705f","_cell_guid":"d2233373-ddd9-4054-90fa-4558c9572b32","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"0994dda9-4720-42f8-a298-cfb2839c1dea","_cell_guid":"692fee9f-a156-48be-87d0-282528f98491","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"b19a6334-5e62-4a90-a1a6-9ad1d2910a5c","_cell_guid":"c03dbed0-b4a3-4e47-8784-ac4714db5218","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"74da67e6-161b-4ed6-843f-f1d4918b0376","_cell_guid":"8a9f6864-6a2d-4124-97f2-e00c6f9d40af","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"3c92eb7c-f8f4-4a54-8918-ebe12d095909","_cell_guid":"a7ee205b-b4c1-4d16-a34e-ebaf9cbb307c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"075ae98c-afbc-439d-8082-f7db859b7a96","_cell_guid":"09f98217-a3b2-4aa3-97d9-ba61699812f8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"f0e279b4-b4dd-4921-9041-733456e20c69","_cell_guid":"89689e7f-2f80-472b-9974-f9a2a1408591","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}